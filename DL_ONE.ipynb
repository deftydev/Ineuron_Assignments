{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UMz9OLWeksG"
      },
      "outputs": [],
      "source": [
        "1. What is the function of a summation junction of a neuron? What is threshold activation\n",
        "function?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "The summation junction of a neuron, also known as the weighted sum or linear combination,\n",
        "computes the sum of the weighted inputs coming into the neuron. Each input is multiplied by its corresponding weight, and the products are summed up. This weighted sum\n",
        "is then passed through the activation function to determine the output of the neuron.\n",
        "The threshold activation function is a type of activation function used in artificial neural networks.\n",
        "It introduces a threshold value or activation threshold that determines whether the neuron should fire (activate) or remain\n",
        "inactive based on the weighted sum of inputs. If the weighted sum exceeds the threshold, the neuron fires and produces an output signal.\n",
        " Otherwise, it remains inactive."
      ],
      "metadata": {
        "id": "m2kOo3aaetIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. What is a step function? What is the difference of step function with threshold function?"
      ],
      "metadata": {
        "id": "tlkezz4wexRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A step function, also known as the Heaviside step function, is a mathematical function that maps an\n",
        "input value to a specific output value based on a defined threshold. It has two possible output values: 0 (inactive)\n",
        "if the input is below the threshold, and 1 (active) if the input is equal to or above the threshold."
      ],
      "metadata": {
        "id": "hS-BedoBezRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Explain the McCulloch–Pitts model of neuron."
      ],
      "metadata": {
        "id": "ZnXDs-iGe3AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The McCulloch–Pitts model of a neuron, proposed by Warren McCulloch and Walter Pitts in 1943,\n",
        "was one of the first mathematical models of an artificial neuron. It aimed to simulate the behavior of biological neurons\n",
        "using simple mathematical operations.\n",
        "In the McCulloch–Pitts model, a neuron receives multiple binary inputs (either on or off) from other neurons or external sources.\n",
        " Each input is assigned a weight that represents its significance or influence. The neuron calculates the weighted sum of inputs and compares\n",
        " it to a threshold value. If the weighted sum exceeds the threshold, the neuron fires and produces an output signal (1),\n",
        " indicating activation. If the weighted sum is below the threshold, the neuron remains inactive and outputs a signal of 0."
      ],
      "metadata": {
        "id": "87g5v9dFe3II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Explain the ADALINE network model."
      ],
      "metadata": {
        "id": "Wae8pxX2e3OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The ADALINE (Adaptive Linear Neuron) network model, developed by Bernard Widrow and Ted Hoff in the late 1950s, is a variation of the\n",
        "original McCulloch–Pitts model. ADALINE introduced the concept of adaptive weights, allowing the network to learn from data and adjust\n",
        "its weights accordingly.\n",
        "In the ADALINE network, similar to other neural networks, inputs are multiplied by their corresponding weights and summed up.\n",
        "However, instead of using a threshold function for activation, the ADALINE model uses a linear activation function.\n",
        "The weighted sum of inputs is passed through the activation function, and the resulting output is compared to a desired target output.\n",
        "The difference between the actual output and the target output, known as the error, is used to adjust the weights through a process\n",
        " called gradient descent.\n",
        "The goal is to minimize the error and achieve better accuracy in predicting the output."
      ],
      "metadata": {
        "id": "15APkT3Ge3QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?\n",
        "\n",
        "The constraint of a simple perceptron is that it can only learn linearly separable patterns.\n",
        "A perceptron applies a linear combination of inputs and passes the result through an activation function to produce an output.\n",
        "It adjusts its weights during training to find the optimal decision boundary that separates different classes of data points.\n",
        "However, if the data is not linearly separable, meaning there is no single straight line or hyperplane that can perfectly separate\n",
        "the classes, a simple perceptron may fail to converge and accurately classify the data.\n",
        "\n",
        "\n",
        "6. What is linearly inseparable problem? What is the role of the hidden layer?\n",
        "A linearly inseparable problem refers to a scenario where classes or patterns in a dataset cannot be separated by a linear decision boundary.\n",
        "In other words, there is no straight line or hyperplane that can perfectly separate the data into distinct classes.\n",
        "This can occur when the relationship between input features and class labels is non-linear or when classes overlap.\n",
        "\n",
        "7. Explain XOR problem in case of a simple perceptron.\n",
        "The XOR problem refers to the inability of a simple perceptron to learn and accurately classify the XOR (exclusive OR) logic function.\n",
        "The XOR function takes two binary inputs (A and B) and returns 1\n",
        "if the inputs are different (one is 0 and the other is 1), and 0 if the inputs are the same (both 0 or both 1).\n",
        "\n",
        "8. Design a multi-layer perceptron to implement A XOR B.\n",
        "\n",
        "To implement A XOR B using a multi-layer perceptron (MLP), we can design a neural network with an input layer, a hidden layer,\n",
        "and an output layer.\n",
        "Here's a possible architecture for an MLP to implement A XOR B:\n",
        "Input Layer: The input layer consists of two nodes, representing the input features A and B.\n",
        "\n",
        "Hidden Layer: The hidden layer contains one or more neurons. Let's assume we have two neurons in the hidden layer.\n",
        "\n",
        "Output Layer: The output layer consists of a single neuron that produces the output representing the XOR of A and B.\n",
        "\n",
        "9. Explain the single-layer feed forward architecture of ANN.\n",
        "The single-layer feed-forward architecture of an artificial neural network (ANN) refers to a network structure where the neurons\n",
        "are arranged in a single layer, with connections only going forward from the input to the output layer.\n",
        "This type of architecture is also known as a perceptron or a single-layer perceptron.\n",
        "\n",
        "10. Explain the competitive network architecture of ANN.\n",
        "The competitive network architecture is a type of artificial neural network (ANN) where neurons within the network compete with each other\n",
        "to become active or \"winners\" based on the input data. These networks are also known as self-organizing maps (SOMs) or Kohonen maps, named\n",
        "after their creator, Teuvo Kohonen.\n",
        "In a competitive network, each neuron in the network is associated with a weight vector that represents a specific class or prototype.\n",
        "During training, the weights are adjusted to match the input data. The winning neuron, or the neuron with the closest weight vector to the input,\n",
        "is activated and becomes the winner."
      ],
      "metadata": {
        "id": "8OFFM6vpe3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPiygWlse3XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the\n",
        "backpropagation algorithm used to train the network. **bold text**\n",
        "\n",
        "Steps in the Backpropagation Algorithm for Training a Multi-layer Feedforward Neural Network:\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "The input data is fed into the network, and the activations of each neuron are calculated layer by layer, starting from the input layer.\n",
        "The weighted sum of inputs is calculated for each neuron and passed through an activation function to produce the output.\n",
        "Calculate Output Error:\n",
        "\n",
        "The error between the network's output and the desired target output is calculated using a suitable error measure, such as mean squared error (MSE) or cross-entropy loss.\n",
        "The error is propagated backward through the network to adjust the weights and biases.\n",
        "Backward Pass (Backpropagation):\n",
        "\n",
        "Starting from the output layer, the error is propagated backward through the network to update the weights and biases.\n",
        "The gradients of the error with respect to the weights and biases of each neuron are calculated using the chain rule of derivatives.\n",
        "Update Weights and Biases:\n",
        "\n",
        "The gradients obtained in the backward pass are used to update the weights and biases of the network.\n",
        "The weights and biases are adjusted in the opposite direction of the gradient using an optimization algorithm, such as gradient descent or its variants.\n",
        "Repeat Steps 1-4:\n",
        "\n",
        "The forward pass, error calculation, backward pass, and weight updates are repeated for a specified number of epochs or until the network converges to a satisfactory level of performance.\n",
        "The backpropagation algorithm iteratively adjusts the weights and biases of the network based on the errors propagated backward. It aims to minimize the difference between the network's predicted output and the desired target output by updating the weights and biases in the direction that reduces the error.\n",
        "\n",
        "\n",
        "\n",
        "12. What are the advantages and disadvantages of neural networks?\n",
        "\n",
        "Advantages and Disadvantages of Neural Networks:\n",
        "Advantages:\n",
        "\n",
        "Neural networks can learn and model complex non-linear relationships between input features and target outputs.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Neural networks require a large amount of training data to avoid overfitting and produce accurate results.\n",
        "Training neural networks can be computationally intensive and time-consuming, especially for large networks and complex tasks.\n",
        "\n",
        "\n",
        "13. Write short notes on any two of the following:\n",
        "\n",
        "1. Biological neuron\n",
        "Biological neurons are the fundamental building blocks of the human brain and nervous system.\n",
        "They receive electrical signals, known as inputs, from other neurons through dendrites.\n",
        "The inputs are integrated in the neuron's cell body or soma.\n",
        "If the integrated input reaches a certain threshold, the neuron generates an electrical signal called an action potential or a spike.\n",
        "\n",
        "\n",
        "2. ReLU function\n",
        "ReLU (Rectified Linear Unit) is an activation function commonly used in neural networks.\n",
        "It is defined as f(x) = max(0, x), where x is the input to the function.\n",
        "ReLU transforms negative input values to zero, while positive values remain unchanged.\n",
        "ReLU is computationally efficient and avoids the vanishing gradient problem associated with some other activation functions.\n",
        "\n",
        "3. Single-layer feed forward ANN\n",
        "4. Gradient descent\n",
        "5. Recurrent networks"
      ],
      "metadata": {
        "id": "s-y5TRGbf--6"
      }
    }
  ]
}