{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5lPrDhehgha"
      },
      "outputs": [],
      "source": [
        "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
        "disadvantages?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Computationally Efficient: High-dimensional datasets require more computational resources and\n",
        " time for analysis. Dimensionality reduction can help in reducing computational complexity, making algorithms more efficient.\n",
        "\n",
        "Noise Reduction: High-dimensional datasets often contain noise or irrelevant features."
      ],
      "metadata": {
        "id": "H2lhCu0siVeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1Ob-48tjCPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. What is the dimensionality curse?\n",
        "The dimensionality curse, also known as the curse of dimensionality, refers to the challenges and issues\n",
        "that arise when dealing with high-dimensional data. As the number of dimensions increases, the amount of\n",
        "data required to cover the feature space becomes exponentially larger.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "32PBark1iVge",
        "outputId": "f4ea3af4-30a4-48d4-cb31-cee9d6569171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f1a344e06250>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ". Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
        "can you go about doing it? If not, what is the reason?\n",
        "\n",
        "Generally, it is not possible to reverse the process of reducing the dimensionality of a dataset and recover\n",
        "the original full-dimensional dataset. This is because dimensionality reduction techniques discard some information during the reduction process.\n",
        "However, in certain cases, it might be possible to reconstruct an approximation\n",
        " of the original dataset using dimensionality reduction techniques that allow for reconstruction, such as Principal Component Analysis (PCA)."
      ],
      "metadata": {
        "id": "ywlxxFsRiVil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOm0TBo6iVlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
      ],
      "metadata": {
        "id": "fyjMb5pciVna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PCA (Principal Component Analysis) is primarily designed to capture linear relationships in the data.\n",
        "Therefore, it may not be the most suitable technique to reduce the dimensionality of a nonlinear dataset with many variables.\n",
        "If the dataset exhibits significant nonlinear relationships,\n",
        "other dimensionality reduction techniques specifically designed for nonlinear data, such as Kernel PCA or t-SNE, may be more appropriate."
      ],
      "metadata": {
        "id": "P2gplV0QiVpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance\n",
        "ratio. What is the number of dimensions that the resulting dataset would have?"
      ],
      "metadata": {
        "id": "g4muhr6HiVrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "When running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio,\n",
        "the resulting dataset would have a reduced number of dimensions.\n",
        "The number of dimensions in the resulting dataset depends on the cumulative explained variance of the retained principal components.\n",
        "To determine the number of dimensions, you need to calculate the cumulative explained variance ratio until it reaches or exceeds\n",
        "the desired percentage (95 percent in this case). This can be done by examining the eigenvalues or the explained_variance_ratio_ attribute\n",
        "of the PCA model."
      ],
      "metadata": {
        "id": "mxxkyBB2iVt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUvDUOLRiVwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
        "\n",
        "Vanilla PCA: Vanilla PCA is suitable for linear datasets or datasets where linear relationships capture the majority of the variation.\n",
        " It is widely used and computationally efficient for reducing dimensionality and capturing the most important features of the data.\n",
        "\n",
        "Incremental PCA: Incremental PCA is useful when dealing with large datasets that cannot fit into memory.\n",
        " It processes the data in mini-batches, making it memory-friendly and allowing for online or incremental learning.\n",
        "\n",
        "Randomized PCA: Randomized PCA is beneficial for large datasets when computational efficiency is a priority.\n",
        " It approximates the principal components using a randomized algorithm, making it faster than vanilla PCA while still providing reasonable accuracy.\n",
        "\n",
        "Kernel PCA: Kernel PCA is suitable for nonlinear datasets where the relationships cannot be effectively captured by linear methods.\n"
      ],
      "metadata": {
        "id": "FlLNk5iHiVzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m--omdDviV1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?"
      ],
      "metadata": {
        "id": "gMPix91JiV3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explained Variance Ratio: This metric measures the amount of variance in the original dataset that is retained by the reduced dimensions.\n",
        " A higher explained variance ratio indicates that the reduced dimensions\n",
        "capture a significant portion of the original dataset's information."
      ],
      "metadata": {
        "id": "li8uS0j2iV6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RF1n801iV8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
      ],
      "metadata": {
        "id": "shyNXk1aiV-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Using two different algorithms can be beneficial when each algorithm captures different aspects of the data or when\n",
        "one algorithm complements the limitations of the other. For example, you can start with a nonlinear dimensionality reduction algorithm\n",
        "like Kernel PCA to capture nonlinear relationships, and\n",
        "then follow it up with a linear dimensionality reduction algorithm like PCA to further reduce the dimensions or capture linear patterns."
      ],
      "metadata": {
        "id": "y9bnVjA_iWB9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}